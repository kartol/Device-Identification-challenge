{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# disable warning but only for sake of rendering the notebook\n",
    "# delete verbosity and warning stuff, I am using some depracated code which could cause trouble in the future\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data files to proper json format\n",
    "def dataset_to_json(old_name, new_name):\n",
    "    with open(old_name, 'r') as f:\n",
    "        with open(new_name,'w') as wf:\n",
    "            wf.write(\"[\")\n",
    "            wf.write(next(f))\n",
    "            for line in f:\n",
    "                wf.write(\",\"+line)\n",
    "            wf.write(\"]\")\n",
    "            \n",
    "            \n",
    "# dataset_to_json(\"dataset/test.json\",\"dataset/test_mod.json\")\n",
    "# dataset_to_json(\"dataset/train.json\",\"dataset/train_mod.json\")\n",
    "\n",
    "\n",
    "with open(\"dataset/test_mod.json\") as f:\n",
    "    test_data = json.load(f)\n",
    "test = json_normalize(test_data)\n",
    "\n",
    "with open(\"dataset/train_mod.json\") as f:\n",
    "    train_data = json.load(f)\n",
    "train = json_normalize(train_data)\n",
    "split = len(train)\n",
    "train = pd.concat([train,test], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssdp_features(data, number_of_most_used = 30):\n",
    "    lists = {'st':[], 'nt':[], 'server':[],'user_agent':[]}\n",
    "    keys = lists.keys()\n",
    "    for ssdps in data.ssdp:\n",
    "        if type(ssdps) != float:\n",
    "            for ssdp in ssdps:\n",
    "                for key in keys:\n",
    "                    if key in ssdp:\n",
    "                        lists[key].append(ssdp[key])\n",
    "\n",
    "    most_used_ssdps = { 'st': sorted(zip(Counter(lists['st']).keys(),Counter(lists['st']).values()), key=lambda x: x[1], reverse=True),\n",
    "                        'nt' : sorted(zip(Counter(lists['nt']).keys(),Counter(lists['nt']).values()), key=lambda x: x[1], reverse=True),\n",
    "                        'server' : sorted(zip(Counter(lists['server']).keys(),Counter(lists['server']).values()), key=lambda x: x[1], reverse=True),\n",
    "                        'user_agent' : sorted(zip(Counter(lists['user_agent']).keys(),Counter(lists['user_agent']).values()), key=lambda x: x[1], reverse=True),\n",
    "                      }\n",
    "\n",
    "    def filter_ssdp(ssdp, key):\n",
    "        def filter_(cell):\n",
    "            if type(cell)!=float:\n",
    "                for each in cell:\n",
    "                    if key in each and ssdp == each[key]:\n",
    "                        return True\n",
    "            return False\n",
    "        return filter_\n",
    "\n",
    "    ssdps = {}\n",
    "    for key, sorted_array in most_used_ssdps.items():\n",
    "        ssdps[key] = [each for each, _ in sorted_array][:number_of_most_used]\n",
    "\n",
    "    features_names = []\n",
    "    for key, array in ssdps.items():\n",
    "        for ssdp in array:\n",
    "            data[key+'_'+ssdp] = 0\n",
    "            data.loc[data.ssdp.apply(filter_ssdp(ssdp, key)), key+'_'+ssdp] = 1\n",
    "            features_names.append(key+'_'+ssdp)\n",
    "    \n",
    "    return features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upnp_services_features(data, number_of_most_used = 60):\n",
    "    services = []\n",
    "    for each in data.upnp:\n",
    "        if type(each)!=float:\n",
    "            for e in each:\n",
    "                if 'services' in e:\n",
    "                    services.extend(e['services'])\n",
    "\n",
    "    most_used_services = sorted(zip(Counter(services).keys(),Counter(services).values()), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    def filter_ser(ser):\n",
    "        def filter_(cell):\n",
    "            if type(cell)!=float:\n",
    "                for each in cell:\n",
    "                    if 'services' in each and ser in each['services']:\n",
    "                        return True\n",
    "            return False\n",
    "        return filter_\n",
    "\n",
    "    srvs = [ser for ser, _ in most_used_services][:number_of_most_used]\n",
    "    features_names = []\n",
    "    for ser in srvs:\n",
    "        data[ser] = 0\n",
    "        data.loc[data.upnp.apply(filter_ser(ser)), ser] = 1\n",
    "        features_names.append(ser)\n",
    "\n",
    "    \n",
    "    return features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upnp_description_features(data):\n",
    "            \n",
    "    # mining data from description\n",
    "    def get_model_descriptions(cell):\n",
    "        descriptions = []\n",
    "        if type(cell)!=float:\n",
    "            for each in cell:\n",
    "                if 'model_description' in each:\n",
    "                    descriptions.append('des_' + each['model_description'].lower())\n",
    "        return descriptions\n",
    "\n",
    "    def filter_des(des):\n",
    "        def filter_(cell):\n",
    "            if type(cell)!=float:\n",
    "                for each in cell:\n",
    "                    if 'model_description' in each and des in each['model_description'].lower():\n",
    "                        return True\n",
    "            return False\n",
    "        return filter_\n",
    "\n",
    "    descs = ['files','dial','sony','lime','device','plugin','personal','roku','wireless','dmrplus','tv', 'network', 'camera', 'video', 'audio', 'bluetooth', 'radio', 'desktop', 'render', 'media', 'dongle', 'sensor', 'ip', 'music', 'netcast', 'home', 'control', 'stream', 'nas', 'server', 'cloud', 'xbox', 'play']\n",
    "    features_names = []\n",
    "    for des in descs:\n",
    "        data['d_'+des] = 0\n",
    "        data.loc[data.upnp.apply(filter_des(des)), 'd_'+des] = 1\n",
    "        features_names.append('d_'+des)\n",
    "    return features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upnp_model_name_features(data, number_of_most_used = 60):\n",
    "    def get_models(cell):\n",
    "        models = []\n",
    "        if type(cell)!=float:\n",
    "            for each in cell:\n",
    "                if 'model_name' in each:\n",
    "                    models.append('mod_' + each['model_name'].lower())\n",
    "        return models\n",
    "\n",
    "    def filter_model(model):\n",
    "        def filter_(cell):\n",
    "            if type(cell)!=float:\n",
    "                for each in cell:\n",
    "                    if 'model_name' in each and each['model_name'].lower() == model:\n",
    "                        return True\n",
    "            return False\n",
    "        return filter_\n",
    "\n",
    "    models = []\n",
    "    for each in data.upnp.apply(lambda x: get_models(x)):\n",
    "        models.extend(each)\n",
    "    most_used_models = sorted(zip(Counter(models).keys(),Counter(models).values()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    models = [model for model, _ in most_used_models][:number_of_most_used]\n",
    "    features_names = []\n",
    "    for model in models:\n",
    "        data[model] = 0\n",
    "        data.loc[data.upnp.apply(filter_model(model.split('_')[1])), model] = 1\n",
    "        features_names.append(model)\n",
    "    return features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upnp_manufacturer_features(data, number_of_most_used = 60):\n",
    "\n",
    "    def get_manufacturer(cell):\n",
    "        manufacturers = []\n",
    "        if type(cell)!=float:\n",
    "            for each in cell:\n",
    "                if 'manufacturer' in each:\n",
    "                    manufacturers.append('man_' + each['manufacturer'].lower())\n",
    "        return manufacturers\n",
    "\n",
    "    manufa = []\n",
    "    for each in data.upnp.apply(lambda x: get_manufacturer(x)):\n",
    "        manufa.extend(each)\n",
    "    most_used_manufa = sorted(zip(Counter(manufa).keys(),Counter(manufa).values()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def filter_manu(manu):\n",
    "        def filter_(cell):\n",
    "            if type(cell)!=float:\n",
    "                for each in cell:\n",
    "                    if 'manufacturer' in each and each['manufacturer'].lower() == manu:\n",
    "                        return True\n",
    "            return False\n",
    "        return filter_\n",
    "\n",
    "    manufas = [manu for manu, _ in most_used_manufa][:number_of_most_used]\n",
    "    features_names = []\n",
    "    for manu in manufas:\n",
    "        data[manu] = 0\n",
    "        data.loc[data.upnp.apply(filter_manu(manu.split('_')[1])), manu] = 1\n",
    "        features_names.append(manu)\n",
    "        \n",
    "    return features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upnp_device_type(data):\n",
    "    import re\n",
    "    def get_types(cell):\n",
    "        cell_types = []\n",
    "        for each in cell:\n",
    "            if 'device_type' in each:\n",
    "                search = re.search(\":device:(.*?):\",each['device_type'])\n",
    "                if search is not None:\n",
    "                    cell_types.append(search.group(1).lower())\n",
    "        return sorted(cell_types)   \n",
    "    def get_type_and_device(data):\n",
    "        return zip(data[data.upnp.notna()].upnp.apply(get_types),data[data.upnp.notna()].device_class)\n",
    "\n",
    "    all_types = []\n",
    "    for device_type, _ in get_type_and_device(data):\n",
    "        all_types.extend(device_type)\n",
    "    all_types = sorted(set(all_types))\n",
    "\n",
    "    features_names = []\n",
    "    for each in all_types:\n",
    "        data[each] = 0\n",
    "        data.loc[ data.upnp.apply(lambda x: True if not type(x) == float and each in get_types(x) else False) , each] = 1\n",
    "        features_names.append(each)\n",
    "        \n",
    "    return features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mac(filter_):\n",
    "    def _filter(mac):\n",
    "        if mac:\n",
    "            if mac.startswith(filter_):\n",
    "                return True\n",
    "        return False\n",
    "    return _filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mac_features(data, number_of_most_used = 100):\n",
    "    macs = data.mac.apply(lambda x: x[3:8])\n",
    "\n",
    "    most_used_macs = sorted(zip(Counter(macs).keys(),Counter(macs).values()), key=lambda x: x[1], reverse=True)\n",
    "    macs = [mac for mac, _ in most_used_macs][:number_of_most_used]\n",
    "    features_names = []\n",
    "    for mac in macs:\n",
    "        data[mac] = 0\n",
    "        data.loc[data.mac.apply(filter_mac(mac)), mac] = 1\n",
    "        features_names.append(mac)\n",
    "\n",
    "    return features_names\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dhcp_text_features(data):\n",
    "    def is_dhcp_smth(cell, smth):\n",
    "        for each in cell:\n",
    "            if 'classid' in each and smth in each['classid']:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    dhcps = ['android','dhcpcd', 'yealink', 'MSFT', 'EMLAB', 'udhcp', 'stb', 'IP-STB', 'Linux']\n",
    "\n",
    "    features_names = []\n",
    "    for dhcp in dhcps:\n",
    "        data[dhcp] = 0\n",
    "        data.loc[data.dhcp.apply(lambda x: is_dhcp_smth(x,dhcp) if type(x) != float else False), dhcp] = 1\n",
    "        features_names.append(dhcp)\n",
    "\n",
    "    return features_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdns_features(data, number_of_most_used = 50):\n",
    "    services = []\n",
    "    for each in data[data.mdns_services.notna()].mdns_services:\n",
    "        services.extend(each)\n",
    "\n",
    "    most_used_mdns = sorted(zip(Counter(services).keys(),Counter(services).values()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "    mdns = [ each for each, _ in most_used_mdns[:number_of_most_used]]\n",
    "    features_names = []\n",
    "    for ident in mdns:\n",
    "        data[ident] = 0\n",
    "        data.loc[ (data.mdns_services.notna()) & (data.mdns_services.apply(lambda x:True if type(x) != float and ident in x else False) ), ident] = 1\n",
    "        features_names.append(ident)\n",
    "    return features_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobile_mac_features(data, number_of_most_used = 30):\n",
    "    m = data[data.device_class == 'MOBILE'].mac.apply(lambda x: x[:8])\n",
    "\n",
    "    mm = sorted(zip(Counter(m).keys(),Counter(m).values()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    m_macs = [mac for mac, _ in mm][:number_of_most_used]\n",
    "\n",
    "    features_names = []\n",
    "    for mac in m_macs:\n",
    "        data[mac] = 0\n",
    "        data.loc[data.mac.apply(filter_mac(mac)), mac] = 1\n",
    "        features_names.append(mac)\n",
    "    return features_names\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voice_assistant_mac_features(data, number_of_most_used = 30):\n",
    "    m = data[data.device_class == 'VOICE_ASSISTANT'].mac.apply(lambda x: x[:8])\n",
    "\n",
    "    mm = sorted(zip(Counter(m).keys(),Counter(m).values()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    m_macs = [mac for mac, _ in mm][:number_of_most_used]\n",
    "\n",
    "    features_names = []\n",
    "    for mac in m_macs:\n",
    "        data[mac] = 0\n",
    "        data.loc[data.mac.apply(filter_mac(mac)), mac] = 1\n",
    "        features_names.append(mac)\n",
    "    return features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ip_features(data, number_of_most_used = 30):\n",
    "    ips = []\n",
    "    for each in data.ip:\n",
    "        ips.append(each.split('.')[0])\n",
    "\n",
    "    most_used_ips = sorted(zip(Counter(ips).keys(),Counter(ips).values()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "    ips = [ each for each, _ in most_used_ips][:number_of_most_used]\n",
    "    \n",
    "    features_names = []\n",
    "    for ip in ips:\n",
    "        data['ip_' + ip] = 0\n",
    "        data.loc[ data['ip'].str.startswith( ip ) , 'ip_' + ip] = 1\n",
    "        features_names.append('ip_' + ip)\n",
    "    \n",
    "    return features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_features(data, number_of_most_used = 60):\n",
    "    def is_port(cell, port, protocol):\n",
    "        for service in cell:\n",
    "            if service['port'] == port and service['protocol'] == protocol:\n",
    "                return True\n",
    "        return False\n",
    "    def filter_port(data, port, protocol):\n",
    "        return data['services'].apply(lambda cell: True if type(cell)!=float and is_port(cell,port, protocol) else False)\n",
    "\n",
    "    all_ports = []\n",
    "    for each in data['services']:\n",
    "        if type(each) != float:\n",
    "            for e in each:\n",
    "                all_ports.append(str(e['port'])+'_'+e['protocol'])\n",
    "\n",
    "    most_used = sorted(zip(Counter(all_ports).keys(),Counter(all_ports).values()), key=lambda x: x[1], reverse=True )\n",
    "\n",
    "    ports = [ 'port_'+each for each, _ in most_used[:number_of_most_used]]\n",
    "\n",
    "    features_names = []\n",
    "    for ident in ports:\n",
    "        port, protocol = ident.split('_')[1:]\n",
    "        data[ident] = 0\n",
    "        data.loc[filter_port(data, int(port), protocol), ident] = 1\n",
    "        features_names.append(ident)\n",
    "\n",
    "    return features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dhcp_params_features(data, number_of_most_used = 60):\n",
    "\n",
    "    params = []\n",
    "    for each in train[train.dhcp.notna()].dhcp:\n",
    "        if 'paramlist' in each[0]:\n",
    "            params.append(each[0]['paramlist'])\n",
    "\n",
    "    most_used_params = sorted(zip(Counter(params).keys(),Counter(params).values()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def is_param(cell, param):\n",
    "        if 'paramlist' in cell[0]:\n",
    "            if cell[0]['paramlist'] == param:\n",
    "                return True\n",
    "        return False\n",
    "    def filter_param(train, param):\n",
    "        return train['dhcp'].apply(lambda cell: True if type(cell)!=float and is_param(cell,param) else False)\n",
    "\n",
    "    params = [ each for each, _ in most_used_params[:number_of_most_used]]\n",
    "\n",
    "    features_names = []\n",
    "    for param in params:\n",
    "        train['par_'+param] = 0\n",
    "        train.loc[ filter_param(train, param) , 'par_' + param] = 1\n",
    "        features_names.append('par_'+param)\n",
    "        \n",
    "    return features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['glob_ad'] = train.mac.apply(lambda x: (int(x[1:2],16)&2)//2 )\n",
    "train['zero_mac'] = train.mac.apply(lambda x: 1 if x == \"00:00:00:00:00:00\" else 0)\n",
    "train['dhcp_nan'] = 0\n",
    "train.loc[ (train['dhcp'].isna()), 'dhcp_nan'] = 1\n",
    "train['mdns_services_nan'] = 0\n",
    "train.loc[train['mdns_services'].isna(), 'mdns_services_nan'] = 1\n",
    "train['upnp_nan'] = 0\n",
    "train.loc[train['upnp'].isna(), 'upnp_nan'] = 1\n",
    "train['ip_valid'] = 0\n",
    "train.loc[train['ip'].str.startswith('10') | train['ip'].str.startswith('192.168') | train['ip'].str.startswith('172'), 'ip_valid'] = 1\n",
    "train['ip_nan'] = 0\n",
    "train.loc[train['ip'] == '', 'ip_nan'] = 1\n",
    "train['ssdp_nan'] = 0\n",
    "train.loc[train['ssdp'].isna(), 'ssdp_nan'] = 1\n",
    "train['n_of_tcp'] = train.services.apply(lambda x: len( [] if x is np.nan else [ a['protocol'] for a in x if a['protocol'] == 'tcp'] ) )\n",
    "train['n_of_udp'] = train.services.apply(lambda x: len( [] if x is np.nan else [ a['protocol'] for a in x if a['protocol'] == 'udp'] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssdp_features(train, number_of_most_used = 30)\n",
    "upnp_services_features(train, number_of_most_used = 60)\n",
    "upnp_description_features(train)\n",
    "upnp_model_name_features(train, number_of_most_used = 60)\n",
    "upnp_manufacturer_features(train, number_of_most_used = 60)\n",
    "upnp_device_type(train)\n",
    "mac_features(train, number_of_most_used = 100)\n",
    "dhcp_text_features(train)\n",
    "mdns_features(train, number_of_most_used = 50)  \n",
    "mobile_mac_features(train, number_of_most_used = 30)\n",
    "# voice_assistant_mac_features(train, number_of_most_used = 30)\n",
    "ip_features(train, number_of_most_used = 30)\n",
    "port_features(train, number_of_most_used = 60)\n",
    "dhcp_params_features(train, number_of_most_used = 60);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected data and model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_default_features(data):\n",
    "    data.drop(['mac','ip','services','device_class','device_id','upnp', 'ssdp', 'mdns_services','dhcp'], axis = 1, inplace = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_train_test():\n",
    "    x = train.copy()\n",
    "    del_default_features(x)\n",
    "    x = pd.get_dummies(x)\n",
    "    y = train[['device_class']].copy()\n",
    "    X_train = x[:split].copy()\n",
    "    y_train = y[:split].copy()\n",
    "    X_val = x[split:].copy()\n",
    "    # use all data for final model.\n",
    "    # for finding right params use test size 0.4 or smth you prefer\n",
    "    return train_test_split(X_train, y_train, test_size=0.0001), X_val\n",
    "\n",
    "\n",
    "(X_train, X_test, y_train, y_test), X_val = get_train_test()\n",
    "# use SMOTE to generate synthetic data because original data are imbalanced\n",
    "X_res, y_res = SMOTE().fit_sample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_res)\n",
    "encoded_y = encoder.transform(y_res)\n",
    "dummy_y = np_utils.to_categorical(encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=X_res.shape[1], activation='relu'))\n",
    "    model.add(Dense(dummy_y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "nn_smote_model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "159926/159926 [==============================] - 5s 34us/step - loss: 0.1981 - acc: 0.9518\n",
      "Epoch 2/15\n",
      "159926/159926 [==============================] - 5s 31us/step - loss: 0.0781 - acc: 0.9757\n",
      "Epoch 3/15\n",
      "159926/159926 [==============================] - 5s 30us/step - loss: 0.0705 - acc: 0.9772\n",
      "Epoch 4/15\n",
      "159926/159926 [==============================] - 5s 32us/step - loss: 0.0667 - acc: 0.9784\n",
      "Epoch 5/15\n",
      "159926/159926 [==============================] - 5s 32us/step - loss: 0.0639 - acc: 0.9786\n",
      "Epoch 6/15\n",
      "159926/159926 [==============================] - 5s 32us/step - loss: 0.0620 - acc: 0.9792\n",
      "Epoch 7/15\n",
      "159926/159926 [==============================] - 5s 32us/step - loss: 0.0605 - acc: 0.9797\n",
      "Epoch 8/15\n",
      "159926/159926 [==============================] - 5s 30us/step - loss: 0.0593 - acc: 0.9798\n",
      "Epoch 9/15\n",
      "159926/159926 [==============================] - 5s 31us/step - loss: 0.0581 - acc: 0.9800\n",
      "Epoch 10/15\n",
      "159926/159926 [==============================] - 5s 30us/step - loss: 0.0571 - acc: 0.9802\n",
      "Epoch 11/15\n",
      "159926/159926 [==============================] - 5s 30us/step - loss: 0.0562 - acc: 0.9805\n",
      "Epoch 12/15\n",
      "159926/159926 [==============================] - 5s 30us/step - loss: 0.0556 - acc: 0.9806\n",
      "Epoch 13/15\n",
      "159926/159926 [==============================] - 5s 30us/step - loss: 0.0549 - acc: 0.9809\n",
      "Epoch 14/15\n",
      "159926/159926 [==============================] - 5s 30us/step - loss: 0.0543 - acc: 0.9811\n",
      "Epoch 15/15\n",
      "159926/159926 [==============================] - 5s 30us/step - loss: 0.0538 - acc: 0.9812\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=10,\n",
    "                              verbose=0, mode='auto')\n",
    "history = nn_smote_model.fit(X_res, dummy_y,\n",
    "                       epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def print_report(model, X_train, y_train):\n",
    "    y_pred_bool = np.argmax(model.predict(X_train.values), axis=1)\n",
    "    print(classification_report(y_train, encoder.inverse_transform(y_pred_bool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          AUDIO       1.00      1.00      1.00      3141\n",
      "   GAME_CONSOLE       0.98      0.99      0.99      2383\n",
      "    GENERIC_IOT       0.99      0.95      0.97      2959\n",
      "HOME_AUTOMATION       1.00      1.00      1.00     12302\n",
      "       IP_PHONE       1.00      1.00      1.00      5193\n",
      "      MEDIA_BOX       0.97      0.93      0.95      6059\n",
      "         MOBILE       0.86      0.98      0.91      1382\n",
      "            NAS       1.00      1.00      1.00      2813\n",
      "             PC       1.00      0.99      0.99      6665\n",
      "        PRINTER       1.00      1.00      1.00      4342\n",
      "   SURVEILLANCE       0.99      1.00      0.99      2037\n",
      "             TV       1.00      0.97      0.98      5712\n",
      "VOICE_ASSISTANT       0.87      0.98      0.93      2912\n",
      "\n",
      "       accuracy                           0.98     57900\n",
      "      macro avg       0.97      0.98      0.98     57900\n",
      "   weighted avg       0.98      0.98      0.98     57900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# here you should use X_train and y_train from train_test_split function\n",
    "# but since I am using whole dataset I will use train data and have biased accuracy\n",
    "print_report(nn_smote_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = encoder.inverse_transform(np.argmax(nn_smote_model.predict(X_val), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Id': test.device_id.array, 'Predicted': prediction}\n",
    "pd.DataFrame(data=d).to_csv('pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
